{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "\n",
    "The goal is to use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data and feature explanations can be found at: https://www.kaggle.com/competitions/titanic/data\n",
    "\n",
    "## Submission\n",
    "\n",
    "We'll use the `test.csv` file to test our model, in which we'll put our results in a `submission.csv` file with 2 columns: `PassengerId` and `Survived`.\n",
    "\n",
    "- `gender_submission.csv` contains a set of predictions that assumes all and only female passengers survived, and is an example of how the submission file should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import data.\n",
    "titanic_data = pd.read_csv(\"./train.csv\")\n",
    "# Select target value.\n",
    "y = titanic_data.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we'll make a simple function to create the `submission.csv` file from the model and features we've decided on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model, usedFeatures):\n",
    "  \"\"\"\n",
    "  Create a `submission.csv` file from the model created from a set of features.\n",
    "  `pd.get_dummies()` will automatically be applied to the test data from `test.csv`.\n",
    "  \"\"\"\n",
    "  test_data = pd.read_csv(\"./test.csv\")\n",
    "  test_X = pd.get_dummies(test_data[usedFeatures])\n",
    "  # Create predictions.\n",
    "  predictions = model.predict(test_X)\n",
    "  # Export predictions.\n",
    "  output_df = pd.DataFrame({ \"PassengerId\": test_data.PassengerId, \"Survived\": predictions })\n",
    "  output_df.to_csv(\"./submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model\n",
    "\n",
    "For our initial model, we'll look at some features that intuitively may correspond to the result.\n",
    "\n",
    "- `Sex`: If we think about history, which was mostly a male-dominated society, we should expect potentially males having a higher survival chance compared to females.\n",
    "- `Age`: With age, we might expect children and older people being left behind.\n",
    "- `Fare`: This is a numeric variable and should correlate to the `Pclass` and `Cabin` variables.\n",
    "\n",
    "Speaking of our model, instead of using the `RandomForestRegressor`, we're using the `RandomForestClassifier` model as we're classifying whether a person survived or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sex         Age        Fare\n",
       "count    891  714.000000  891.000000\n",
       "unique     2         NaN         NaN\n",
       "top     male         NaN         NaN\n",
       "freq     577         NaN         NaN\n",
       "mean     NaN   29.699118   32.204208\n",
       "std      NaN   14.526497   49.693429\n",
       "min      NaN    0.420000    0.000000\n",
       "25%      NaN   20.125000    7.910400\n",
       "50%      NaN   28.000000   14.454200\n",
       "75%      NaN   38.000000   31.000000\n",
       "max      NaN   80.000000  512.329200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of features we want to use.\n",
    "features = [\"Sex\", \"Age\", \"Fare\"]\n",
    "\n",
    "# Select columns corresponding to features.\n",
    "X = titanic_data[features]\n",
    "X.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From observing the statistical results, we see that some entries are missing an `Age` value. However, since [version 1.4 of `scikit-learn`, they now support missing values](https://scikit-learn.org/dev/whats_new/v1.4.html#id7). In addition, since we're using a `RandomForestClassifier`, we need to have the `Sex` column contain numeric values, or alternatively, create features based on the values in the column. This can be done by using `get_dummies()`, which will create a boolean `Sex_female` and `Sex_male` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>577</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.699118</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.526497</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.125000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age        Fare Sex_female Sex_male\n",
       "count   714.000000  891.000000        891      891\n",
       "unique         NaN         NaN          2        2\n",
       "top            NaN         NaN      False     True\n",
       "freq           NaN         NaN        577      577\n",
       "mean     29.699118   32.204208        NaN      NaN\n",
       "std      14.526497   49.693429        NaN      NaN\n",
       "min       0.420000    0.000000        NaN      NaN\n",
       "25%      20.125000    7.910400        NaN      NaN\n",
       "50%      28.000000   14.454200        NaN      NaN\n",
       "75%      38.000000   31.000000        NaN      NaN\n",
       "max      80.000000  512.329200        NaN      NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert unique string values in columns to boolean columns.\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "X.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our initial model design and initial submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model.\n",
    "rf_model = RandomForestClassifier(random_state=1)\n",
    "rf_model.fit(X, y)\n",
    "# Create submission.\n",
    "create_submission(rf_model, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On submission, we received a score of `0.73684`, which isn't that bad of an initial guess (note that getting a score of `1` indicates a perfect match).\n",
    "\n",
    "# 2nd Model\n",
    "\n",
    "This time, we'll add on the `Pclass` feature, which indicates the social-economic status of the person and see if anything changes. This feature might add some more context as if the person is of a higher status, they might have a higher priority on entering the lifeboats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Pclass\", \"Sex\", \"Age\", \"Fare\"]\n",
    "X = pd.get_dummies(titanic_data[features])\n",
    "# Create and train model.\n",
    "rf_model = RandomForestClassifier(random_state=1)\n",
    "rf_model.fit(X, y)\n",
    "# Create submission.\n",
    "create_submission(rf_model, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On submission, we received a score of `0.74162`, which is slightly better than what we had prior.\n",
    "\n",
    "# 3rd Model\n",
    "\n",
    "Although we learned that random forests make a good prediction with the default parameters compared to decision trees (ie: we don't need to specify some max depth value), it might be helpful to see what happens if we do limit the tree depth.\n",
    "\n",
    "In the [`Titanic Tutorial`](https://www.kaggle.com/code/alexisbcook/titanic-tutorial), they limited their tree to a depth of 5, so we'll do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model.\n",
    "rf_model = RandomForestClassifier(max_depth=5, random_state=1)\n",
    "rf_model.fit(X, y)\n",
    "# Create submission.\n",
    "create_submission(rf_model, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On submission, we received a score of `0.77751`, which is a substantial improvement.\n",
    "\n",
    "# 4th Model\n",
    "\n",
    "Since we saw an improvement when using a `max_depth` of 5, is there a better value that might result in a better result? Let's try something larger, like 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model.\n",
    "rf_model = RandomForestClassifier(max_depth=7, random_state=1)\n",
    "rf_model.fit(X, y)\n",
    "# Create submission.\n",
    "create_submission(rf_model, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On submission, we received a score of `0.78229`, which is a slight improvement.\n",
    "\n",
    "# 5th Model\n",
    "\n",
    "One last idea that I have that may result in an improved score (without changing the model used) is to maybe take in account the `Embarked` feature. This might contribute to the final result as if you embarked from the first port, you may have gotten a better cabin that's closer to the lifeboats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\"]\n",
    "X = pd.get_dummies(titanic_data[features])\n",
    "# Create and train model.\n",
    "rf_model = RandomForestClassifier(max_depth=7, random_state=1)\n",
    "rf_model.fit(X, y)\n",
    "# Create submission.\n",
    "create_submission(rf_model, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On submission, we received a score of `0.78468`, which is a slight improvement. Back to the topic of `max_depth`, since we changed our parameters, the value of `max_depth` that maximizes our accuracy score might change. Let's go back to using a `max_depth` of 5 and see if there's any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model.\n",
    "rf_model = RandomForestClassifier(max_depth=5, random_state=1)\n",
    "rf_model.fit(X, y)\n",
    "# Create submission.\n",
    "create_submission(rf_model, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this change, we obtained a score of `0.79186`, which indicates that yes, the parameters used will change based on the features selected.\n",
    "\n",
    "# 6th Model\n",
    "\n",
    "After finishing kaggle Learn's [\"Intermediate Machine Learning\"](https://www.kaggle.com/learn/intermediate-machine-learning) course, let's see if using more advanced techniques such as XGBoost can help improve our accuracy score. In addition, we'll utilize pipelines to clean up the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.66584\n",
      "[1]\tvalidation_0-logloss:0.64601\n",
      "[2]\tvalidation_0-logloss:0.62825\n",
      "[3]\tvalidation_0-logloss:0.61281\n",
      "[4]\tvalidation_0-logloss:0.59845\n",
      "[5]\tvalidation_0-logloss:0.58568\n",
      "[6]\tvalidation_0-logloss:0.57368\n",
      "[7]\tvalidation_0-logloss:0.56346\n",
      "[8]\tvalidation_0-logloss:0.55366\n",
      "[9]\tvalidation_0-logloss:0.54473\n",
      "[10]\tvalidation_0-logloss:0.53561\n",
      "[11]\tvalidation_0-logloss:0.52840\n",
      "[12]\tvalidation_0-logloss:0.52113\n",
      "[13]\tvalidation_0-logloss:0.51492\n",
      "[14]\tvalidation_0-logloss:0.50910\n",
      "[15]\tvalidation_0-logloss:0.50412\n",
      "[16]\tvalidation_0-logloss:0.49929\n",
      "[17]\tvalidation_0-logloss:0.49519\n",
      "[18]\tvalidation_0-logloss:0.49155\n",
      "[19]\tvalidation_0-logloss:0.48835\n",
      "[20]\tvalidation_0-logloss:0.48499\n",
      "[21]\tvalidation_0-logloss:0.48297\n",
      "[22]\tvalidation_0-logloss:0.48023\n",
      "[23]\tvalidation_0-logloss:0.47835\n",
      "[24]\tvalidation_0-logloss:0.47641\n",
      "[25]\tvalidation_0-logloss:0.47435\n",
      "[26]\tvalidation_0-logloss:0.47229\n",
      "[27]\tvalidation_0-logloss:0.47127\n",
      "[28]\tvalidation_0-logloss:0.46962\n",
      "[29]\tvalidation_0-logloss:0.46905\n",
      "[30]\tvalidation_0-logloss:0.46783\n",
      "[31]\tvalidation_0-logloss:0.46737\n",
      "[32]\tvalidation_0-logloss:0.46653\n",
      "[33]\tvalidation_0-logloss:0.46626\n",
      "[34]\tvalidation_0-logloss:0.46532\n",
      "[35]\tvalidation_0-logloss:0.46567\n",
      "[36]\tvalidation_0-logloss:0.46595\n",
      "[37]\tvalidation_0-logloss:0.46644\n",
      "[38]\tvalidation_0-logloss:0.46627\n",
      "[39]\tvalidation_0-logloss:0.46519\n",
      "[40]\tvalidation_0-logloss:0.46583\n",
      "[41]\tvalidation_0-logloss:0.46519\n",
      "[42]\tvalidation_0-logloss:0.46477\n",
      "[43]\tvalidation_0-logloss:0.46476\n",
      "[44]\tvalidation_0-logloss:0.46410\n",
      "[45]\tvalidation_0-logloss:0.46485\n",
      "[46]\tvalidation_0-logloss:0.46420\n",
      "[47]\tvalidation_0-logloss:0.46505\n",
      "[48]\tvalidation_0-logloss:0.46559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Reset the information since we converted `Sex` to dummy features earlier on.\n",
    "X = titanic_data[features]\n",
    "\n",
    "# Split data to train & validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Create preprocessing logic for numeric & categorical features used.\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "  (\"num\", SimpleImputer(strategy=\"median\"), [\"Pclass\", \"Age\", \"Fare\"]),\n",
    "  (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"Sex\", \"Embarked\"])\n",
    "])\n",
    "\n",
    "X_train_transform = preprocessor.fit_transform(X_train)\n",
    "X_valid_transform = preprocessor.transform(X_valid)\n",
    "\n",
    "# Use XGBoost to get the best results.\n",
    "xg_model = XGBClassifier(n_estimators=1000, learning_rate=0.05, early_stopping_rounds=5, random_state=1)\n",
    "xg_model.fit(X_train_transform, y_train, eval_set=[(X_valid_transform, y_valid)])\n",
    "\n",
    "# Predict results.\n",
    "test_data = pd.read_csv(\"./test.csv\")\n",
    "predictions = xg_model.predict(preprocessor.transform(test_data))\n",
    "# Export predictions.\n",
    "output_df = pd.DataFrame({ \"PassengerId\": test_data.PassengerId, \"Survived\": predictions })\n",
    "output_df.to_csv(\"./submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this more \"complex\" model, we obtained a score of `0.78708`, which is slightly worse than before, but can be improved as some fine-tuning is required to get the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def use_XGB(strategy, learning_rate):\n",
    "  # Create preprocessing logic for numeric & categorical features used.\n",
    "  preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", SimpleImputer(strategy=strategy), [\"Pclass\", \"Age\", \"Fare\"]),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"Sex\", \"Embarked\"])\n",
    "  ])\n",
    "\n",
    "  X_train_transform = preprocessor.fit_transform(X_train)\n",
    "  X_valid_transform = preprocessor.transform(X_valid)\n",
    "\n",
    "  # Use XGBoost to get the best results.\n",
    "  xg_model = XGBClassifier(n_estimators=1000, learning_rate=learning_rate, early_stopping_rounds=5, random_state=1)\n",
    "  xg_model.fit(X_train_transform, y_train, eval_set=[(X_valid_transform, y_valid)], verbose=False)\n",
    "  # See accuracy of model with these parameters.\n",
    "  valid_predictions = xg_model.predict(preprocessor.transform(X_valid))\n",
    "  mae = mean_absolute_error(y_valid, valid_predictions)\n",
    "  print(\"Obtained MAE score of `{}` from parameters: `strategy={}`, `learning_rate={}`\".format(mae, strategy, learning_rate))\n",
    "\n",
    "# Parameters that we model XBG with.\n",
    "all_strategies = [\"mean\", \"median\", \"most_frequent\", \"constant\"]\n",
    "all_learning_rates = [0.01, 0.03, 0.05, 0.07, 0.1, 0.25, 0.5]\n",
    "\n",
    "# for strategy in all_strategies:\n",
    "#   for learning_rate in all_learning_rates:\n",
    "#     use_XGB(strategy, learning_rate)\n",
    "\n",
    "# for learning_rate in [i * 0.01 for i in range(0, 100)]:\n",
    "#   use_XGB(\"median\", learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
