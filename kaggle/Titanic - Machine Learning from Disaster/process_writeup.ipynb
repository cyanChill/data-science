{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "\n",
    "The goal is to use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data and feature explanations can be found at: https://www.kaggle.com/competitions/titanic/data\n",
    "\n",
    "## Submission\n",
    "\n",
    "We'll use the `test.csv` file to test our model, in which we'll put our results in a `submission.csv` file with 2 columns: `PassengerId` and `Survived`.\n",
    "\n",
    "- `gender_submission.csv` contains a set of predictions that assumes all and only female passengers survived, and is an example of how the submission file should look like.\n",
    "\n",
    "We'll make a simple function to create the `submission.csv` file from the model and features we've decided on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import data.\n",
    "titanic_data = pd.read_csv(\"./train.csv\")\n",
    "# Select target value.\n",
    "X = titanic_data.copy()\n",
    "y = X.pop(\"Survived\")\n",
    "\n",
    "def create_submission(model, transformer):\n",
    "  \"\"\"Create a `submission.csv` file from the model created from a set offeatures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Model\n",
    "      Model we want to test.\n",
    "    transformer : callable\n",
    "      Function that transforms the test data, returning a DataFrame with only the columns used.\n",
    "  \"\"\"\n",
    "  test_data = pd.read_csv(\"./test.csv\")\n",
    "  # Create predictions.\n",
    "  predictions = model.predict(transformer(test_data))\n",
    "  # Export predictions.\n",
    "  output_df = pd.DataFrame({ \"PassengerId\": test_data.PassengerId, \"Survived\": predictions })\n",
    "  output_df.to_csv(\"./submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model\n",
    "\n",
    "For our initial model, we'll look at some features that intuitively may correspond to the result.\n",
    "\n",
    "- `Sex`: If we think about history, which was mostly a male-dominated society, we should expect potentially males having a higher survival chance compared to females.\n",
    "- `Age`: With age, we might expect children and older people being left behind.\n",
    "- `Fare`: This is a numeric variable and should correlate to the `Pclass` and `Cabin` variables.\n",
    "\n",
    "Speaking of our model, instead of using the `RandomForestRegressor`, we're using the `RandomForestClassifier` model as we're classifying whether a person survived or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sex         Age        Fare\n",
       "count    891  714.000000  891.000000\n",
       "unique     2         NaN         NaN\n",
       "top     male         NaN         NaN\n",
       "freq     577         NaN         NaN\n",
       "mean     NaN   29.699118   32.204208\n",
       "std      NaN   14.526497   49.693429\n",
       "min      NaN    0.420000    0.000000\n",
       "25%      NaN   20.125000    7.910400\n",
       "50%      NaN   28.000000   14.454200\n",
       "75%      NaN   38.000000   31.000000\n",
       "max      NaN   80.000000  512.329200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of features we want to use.\n",
    "feats_1 = [\"Sex\", \"Age\", \"Fare\"]\n",
    "# Select columns corresponding to features.\n",
    "X_1 = X[feats_1]\n",
    "\n",
    "X_1.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From observing the statistical results, we see that some entries are missing an `Age` value. However, since [version 1.4 of `scikit-learn`, they now support missing values](https://scikit-learn.org/dev/whats_new/v1.4.html#id7). In addition, since we're using a `RandomForestClassifier`, we need to have the `Sex` column contain numeric values, or alternatively, create features based on the values in the column. This can be done by using `get_dummies()`, which will create a boolean `Sex_female` and `Sex_male` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>577</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.699118</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.526497</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>20.125000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age        Fare Sex_female Sex_male\n",
       "count   714.000000  891.000000        891      891\n",
       "unique         NaN         NaN          2        2\n",
       "top            NaN         NaN      False     True\n",
       "freq           NaN         NaN        577      577\n",
       "mean     29.699118   32.204208        NaN      NaN\n",
       "std      14.526497   49.693429        NaN      NaN\n",
       "min       0.420000    0.000000        NaN      NaN\n",
       "25%      20.125000    7.910400        NaN      NaN\n",
       "50%      28.000000   14.454200        NaN      NaN\n",
       "75%      38.000000   31.000000        NaN      NaN\n",
       "max      80.000000  512.329200        NaN      NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert unique string values in columns to boolean columns.\n",
    "X_1 = pd.get_dummies(X_1)\n",
    "\n",
    "X_1.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our initial model design and initial submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(\n",
    "  model=RandomForestClassifier(random_state=1).fit(X_1, y),\n",
    "  transformer=lambda data: pd.get_dummies(data[feats_1])\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On submission, we received a score of `0.73684`, which isn't that bad of an initial guess (note that getting a score of `1` indicates a perfect match).\n",
    "\n",
    "# 2nd Model\n",
    "\n",
    "This time, we'll add on the `Pclass` feature, which indicates the social-economic status of the person and see if anything changes. This feature might add some more context as if the person is of a higher status, they might have a higher priority on entering the lifeboats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_2 = [\"Pclass\", \"Sex\", \"Age\", \"Fare\"]\n",
    "X_2 = pd.get_dummies(X[feats_2])\n",
    "\n",
    "create_submission(\n",
    "  model=RandomForestClassifier(random_state=1).fit(X_2, y),\n",
    "  transformer=lambda data: pd.get_dummies(data[feats_2])\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On submission, we received a score of `0.74162`, which is slightly better than what we had prior.\n",
    "\n",
    "# 3rd Model\n",
    "\n",
    "Although we learned that random forests make a good prediction with the default parameters compared to decision trees (ie: we don't need to specify some max depth value), it might be helpful to see what happens if we do limit the tree depth.\n",
    "\n",
    "In the [`Titanic Tutorial`](https://www.kaggle.com/code/alexisbcook/titanic-tutorial), they limited their tree to a depth of 5, so we'll do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(\n",
    "  model=RandomForestClassifier(max_depth=5, random_state=1).fit(X_2, y),\n",
    "  transformer=lambda data: pd.get_dummies(data[feats_2])\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On submission, we received a score of `0.77751`, which is a substantial improvement.\n",
    "\n",
    "Since we saw an improvement when specifying a `max_depth` of 5, is there a better value that might result in a better result? Let's try something larger, like 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(\n",
    "  model=RandomForestClassifier(max_depth=7, random_state=1).fit(X_2, y),\n",
    "  transformer=lambda data: pd.get_dummies(data[feats_2])\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On submission, we received a score of `0.78229`, which is a slight improvement.\n",
    "\n",
    "# 4th Model\n",
    "\n",
    "One last idea that I have that may result in an improved score (without changing the model used) is to maybe take in account the `Embarked` feature. This might contribute to the final result as if you embarked from the first port, you may have gotten a better cabin that's closer to the lifeboats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_4 = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\"]\n",
    "X_4 = pd.get_dummies(X[feats_4])\n",
    "\n",
    "create_submission(\n",
    "  model=RandomForestClassifier(max_depth=7, random_state=1).fit(X_4, y),\n",
    "  transformer=lambda data: pd.get_dummies(data[feats_4])\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On submission, we received a score of `0.78468`, which is a slight improvement. Back to the topic of `max_depth`, since we changed our parameters, the value of `max_depth` that maximizes our accuracy score might change. Let's go back to using a `max_depth` of 5 and see if there's any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(\n",
    "  model=RandomForestClassifier(max_depth=5, random_state=1).fit(X_4, y),\n",
    "  transformer=lambda data: pd.get_dummies(data[feats_4])\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this change, we obtained a score of `0.79186`, which indicates that yes, the parameters used will change based on the features selected.\n",
    "\n",
    "# 5th Model\n",
    "\n",
    "After finishing kaggle Learn's [\"Intermediate Machine Learning\"](https://www.kaggle.com/learn/intermediate-machine-learning) course, let's see if using more advanced techniques such as XGBoost can help improve our accuracy score. In addition, we'll utilize pipelines to clean up the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.66584\n",
      "[1]\tvalidation_0-logloss:0.64601\n",
      "[2]\tvalidation_0-logloss:0.62825\n",
      "[3]\tvalidation_0-logloss:0.61281\n",
      "[4]\tvalidation_0-logloss:0.59845\n",
      "[5]\tvalidation_0-logloss:0.58568\n",
      "[6]\tvalidation_0-logloss:0.57368\n",
      "[7]\tvalidation_0-logloss:0.56346\n",
      "[8]\tvalidation_0-logloss:0.55366\n",
      "[9]\tvalidation_0-logloss:0.54473\n",
      "[10]\tvalidation_0-logloss:0.53561\n",
      "[11]\tvalidation_0-logloss:0.52840\n",
      "[12]\tvalidation_0-logloss:0.52113\n",
      "[13]\tvalidation_0-logloss:0.51492\n",
      "[14]\tvalidation_0-logloss:0.50910\n",
      "[15]\tvalidation_0-logloss:0.50412\n",
      "[16]\tvalidation_0-logloss:0.49929\n",
      "[17]\tvalidation_0-logloss:0.49519\n",
      "[18]\tvalidation_0-logloss:0.49155\n",
      "[19]\tvalidation_0-logloss:0.48835\n",
      "[20]\tvalidation_0-logloss:0.48499\n",
      "[21]\tvalidation_0-logloss:0.48297\n",
      "[22]\tvalidation_0-logloss:0.48023\n",
      "[23]\tvalidation_0-logloss:0.47835\n",
      "[24]\tvalidation_0-logloss:0.47641\n",
      "[25]\tvalidation_0-logloss:0.47435\n",
      "[26]\tvalidation_0-logloss:0.47229\n",
      "[27]\tvalidation_0-logloss:0.47127\n",
      "[28]\tvalidation_0-logloss:0.46962\n",
      "[29]\tvalidation_0-logloss:0.46905\n",
      "[30]\tvalidation_0-logloss:0.46783\n",
      "[31]\tvalidation_0-logloss:0.46737\n",
      "[32]\tvalidation_0-logloss:0.46653\n",
      "[33]\tvalidation_0-logloss:0.46626\n",
      "[34]\tvalidation_0-logloss:0.46532\n",
      "[35]\tvalidation_0-logloss:0.46567\n",
      "[36]\tvalidation_0-logloss:0.46595\n",
      "[37]\tvalidation_0-logloss:0.46644\n",
      "[38]\tvalidation_0-logloss:0.46627\n",
      "[39]\tvalidation_0-logloss:0.46519\n",
      "[40]\tvalidation_0-logloss:0.46583\n",
      "[41]\tvalidation_0-logloss:0.46519\n",
      "[42]\tvalidation_0-logloss:0.46477\n",
      "[43]\tvalidation_0-logloss:0.46476\n",
      "[44]\tvalidation_0-logloss:0.46410\n",
      "[45]\tvalidation_0-logloss:0.46485\n",
      "[46]\tvalidation_0-logloss:0.46420\n",
      "[47]\tvalidation_0-logloss:0.46505\n",
      "[48]\tvalidation_0-logloss:0.46559\n",
      "[49]\tvalidation_0-logloss:0.46602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split data to train & validation sets.\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Create preprocessing logic for numeric & categorical features used.\n",
    "preprocessor_5 = ColumnTransformer(transformers=[\n",
    "  (\"num\", SimpleImputer(strategy=\"median\"), [\"Pclass\", \"Age\", \"Fare\"]),\n",
    "  (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"Sex\", \"Embarked\"])\n",
    "])\n",
    "\n",
    "X_train_transform = preprocessor_5.fit_transform(X_train)\n",
    "X_valid_transform = preprocessor_5.transform(X_valid)\n",
    "\n",
    "# Use XGBoost to get the best results.\n",
    "xg_model = XGBClassifier(n_estimators=1000, learning_rate=0.05, early_stopping_rounds=5, random_state=1)\n",
    "xg_model.fit(X_train_transform, y_train, eval_set=[(X_valid_transform, y_valid)])\n",
    "\n",
    "# Create submission.\n",
    "create_submission(model=xg_model, transformer=preprocessor_5.transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this more \"complex\" model, we obtained a score of `0.78708`, which is slightly worse than before, but can be improved as some fine-tuning is required to get the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def use_XGB(strategy, learning_rate):\n",
    "  # Create preprocessing logic for numeric & categorical features used.\n",
    "  preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", SimpleImputer(strategy=strategy), [\"Pclass\", \"Age\", \"Fare\"]),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [\"Sex\", \"Embarked\"])\n",
    "  ])\n",
    "\n",
    "  X_train_transform = preprocessor.fit_transform(X_train)\n",
    "  X_valid_transform = preprocessor.transform(X_valid)\n",
    "\n",
    "  # Use XGBoost to get the best results.\n",
    "  xg_model = XGBClassifier(n_estimators=1000, learning_rate=learning_rate, early_stopping_rounds=5, random_state=1)\n",
    "  xg_model.fit(X_train_transform, y_train, eval_set=[(X_valid_transform, y_valid)], verbose=False)\n",
    "  # See accuracy of model with these parameters.\n",
    "  valid_predictions = xg_model.predict(preprocessor.transform(X_valid))\n",
    "  mae = mean_absolute_error(y_valid, valid_predictions)\n",
    "  print(\"Obtained MAE score of `{}` from parameters: `strategy={}`, `learning_rate={}`\".format(mae, strategy, learning_rate))\n",
    "\n",
    "# Parameters that we model XBG with.\n",
    "all_strategies = [\"mean\", \"median\", \"most_frequent\", \"constant\"]\n",
    "all_learning_rates = [0.01, 0.03, 0.05, 0.07, 0.1, 0.25, 0.5]\n",
    "\n",
    "# for strategy in all_strategies:\n",
    "#   for learning_rate in all_learning_rates:\n",
    "#     use_XGB(strategy, learning_rate)\n",
    "\n",
    "# for learning_rate in [i * 0.01 for i in range(0, 100)]:\n",
    "#   use_XGB(\"median\", learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From running testing different XGB models with different parameter values using the function created above, we landed on with essentially the best model being the one we've used previously. From this, it seems that we may need to do some feature engineering to recieve a higher score.\n",
    "\n",
    "# 6th Model\n",
    "\n",
    "With this model, we plan on doing some feature engineering in hopes of obtaining a better score. With that in mind, we'll go back to using the Random Forest model instead of our XGB model as our best score did come from a Random Forest.\n",
    "\n",
    "Some potential feature ideas that I had include:\n",
    "- Binning the `Age` & `Fare` categories.\n",
    "- Creating a new feature based on the 1st letter of `Cabin`.\n",
    "\n",
    "#### `Cabin_Level` Feature\n",
    "\n",
    "This feature is derived from the existing `Cabin` feature and indicates the \"level\" on the Titanic their cabin is in. According to diagrams of the titanic, the levels start from `A` at the top to `G` at the bottom. We do notice that there's a rogue `T` entry, which we'll replace with `nan` afterwards.\n",
    "\n",
    "Although the `Cabin` feature is populated for a little under 1/4 of the training data, this might be some critical missing information due to the dataset being small.\n",
    "\n",
    "#### `Fare` Binning\n",
    "\n",
    "One problem we have with binning the `Fare` is that the distribution is not as even, so the boundaries might not be that good.\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "sns.histplot(data=X, x=\"Fare\")\n",
    "```\n",
    "\n",
    "There's something else we could do instead with the `Fare` feature which is standardizing it using `StandardScaler`. From viewing the distribution, we see that we have a very wide range of values, ranging from 0 to 512. Compared to the other numeric values, the scale is way off. This poses an issue with the model as it may factor heavily on this feature. Standardizing the data will scale the values down, making them more comparable.\n",
    "\n",
    "#### `Age` Binning\n",
    "\n",
    "`Age` is something that can be binned easily and manually as there's a clear methodology of how we can do this. With the binning of the `Age` feature, we can use a \"common age range\" from that time period. We specifically choose an age range around that time period as the average life expectancy from back then and now are way different (late 40s vs late 70s). Our age range will look something along the lines of:\n",
    "- 0-9\n",
    "- 10-15\n",
    "- 16-25\n",
    "- 26-45\n",
    "- 45+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feats_6 = [\"Pclass\", \"Sex\", \"Age_binned\", \"Fare_normal\", \"Embarked\", \"Cabin_Level\"]\n",
    "\n",
    "cabin_levels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "cabin_levels_cols = [f\"Cabin_Level_{letter}\" for letter in cabin_levels]\n",
    "\n",
    "# Bin \"Age\".\n",
    "def bin_age(age):\n",
    "  if age > 45:\n",
    "    return 5\n",
    "  elif age > 25:\n",
    "    return 4\n",
    "  elif age > 15:\n",
    "    return 3\n",
    "  elif age > 9:\n",
    "    return 2\n",
    "  return 1\n",
    "\n",
    "def transform_6(data):\n",
    "  data[\"Age_binned\"] = data[\"Age\"].map(bin_age)\n",
    "  # Ensure we have all the valid cabin levels.\n",
    "  data.loc[:, cabin_levels_cols] = 0\n",
    "  data[\"Cabin_Level\"] = data[\"Cabin\"].str[0]\n",
    "  # Remove invalid cabin level values.\n",
    "  data.loc[~data[\"Cabin_Level\"].isin(cabin_levels), \"Cabin_Level\"] = np.nan\n",
    "  # Standardize fare.\n",
    "  scaler = StandardScaler()\n",
    "  data[\"Fare_normal\"] = scaler.fit_transform(data[[\"Fare\"]])\n",
    "  return pd.get_dummies(data[feats_6])\n",
    "\n",
    "create_submission(\n",
    "  model=RandomForestClassifier(max_depth=5, random_state=1).fit(transform_6(X), y),\n",
    "  transformer=lambda data: transform_6(data)\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this feature engineering applied, we obtained a score of `0.80143`, which is a great improvement as we broke through the 80% accuracy point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
